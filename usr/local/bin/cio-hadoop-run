#!/bin/bash

# Copyright (c) 2016 Coho Data Inc.
# The subject matter distributed under this license is or is based on
# information and material generated by Coho Data Inc. It may only be
# acquired, used, modified and distributed under the terms of the Coho
# Data Compute Cluster License v1.0.  Except as permitted in the Coho
# Data Compute Cluster License v1.0, all other rights are reserved in
# any copyright or other similar rights which may exist. Execution of
# software distributed under this Coho Data Compute Cluster License
# v1.0 may cause you to acquire third-party software (as described in
# the accompanying documentation) and you agree (a) to comply with the
# applicable licenses thereunder and (b) that Coho is not responsible
# in any way for your compliance or non-compliance with the applicable
# third-party licenses or the consequences of your being subject to
# said licenses or your compliance or non-compliance.

# Change logger configuration for YARN
sed -i "s/LOGGER_ENV_VAR/${LOGGER_ENV_VAR}/" /etc/hadoop/conf/log4j.properties

# if true, create a local directory for nodemanager spill data
make_local_dir=false
service_in_backround=false

case $ROLE in
  resourcemanager)
    service_script=hadoop-yarn-resourcemanager
    ;;
  historyserver)
    service_script=hadoop-mapreduce-historyserver
    ;;
  nodemanager)
    service_script=hadoop-yarn-nodemanager
    local_dir_base='/mnt/nodemanager'
    local_dir_conf_file='/etc/hadoop/conf/yarn-site.xml'
    make_local_dir=true
    ;;
  spark-master)
    service_script=spark-master
    service_in_backround=true
    service_log_file='/var/log/spark/spark-master.out'
    service_pid_file='/var/run/spark/spark-master.pid'
    ;;
  spark-worker)
    service_script=spark-worker
    local_dir_base='/mnt/spark-worker'
    local_dir_conf_file='/etc/spark/conf/spark-env.sh'
    make_local_dir=true
    service_in_backround=true
    service_log_file='/var/log/spark/spark-worker.out'
    service_pid_file='/var/run/spark/spark-worker.pid'
    ;;
  *)
    echo "Role $ROLE is not supported."
    echo "Supported roles: resourcemanager, nodemanager, historyserver, spark-master, spark-worker"
    exit 1
  ;;
esac

spawn_service_poll() {
    # We can't wait for the pid, just poll until it dies
    while [ -f $service_pid_file ]
    do
        pid=$(cat $service_pid_file)
        if [ $(ps -o pid= -p $pid) ];
        then
            sleep 3
        else
            break
        fi
    done
    # Not the best way to kill another process, but very handy
    killall tail
}

IP=$(ip addr show | grep "inet " | awk '{print $2}' | sed '/^127.0.0.1/ d' | cut -d/ -f1)
if [[ -z "$IP" ]]; then
  echo "Container does not have an IP."
  echo "Exiting."
  exit 1
fi

if [[ -z "$COHO_TENANT" ]]; then
  echo "COHO_TENANT not specified; not writing rack location."
  WITH_COHO_HADOOP_TOPOLOGY=false
fi
if [[ -z "$COHO_CLUSTER_UUID" ]]; then
  echo "COHO_CLUSTER_UUID not specified; not writing rack location."
  WITH_COHO_HADOOP_TOPOLOGY=false
fi
if [[ -z "$COHO_NODE_UUID" ]]; then
  echo "COHO_NODE_UUID not specified; not writing rack location."
  WITH_COHO_HADOOP_TOPOLOGY=false
fi

# Write this container's location into the topology mapping.
# Note that we write two entries, one with the container's IP and another
# with the container's short hostname because the resource manager has been
# observed to resolve NodeManager locations by both name and IP.
NAME=$(hostname -s)
RACK="coho/${COHO_TENANT}/hdfs/topology/${COHO_CLUSTER_UUID}/${COHO_NODE_UUID}"
LOCATION_IP="${RACK}/${IP}"
LOCATION_NAME="${RACK}/${NAME}"
CTIME=$(date +%s)


# Preserve COHO environment variables by writing them to /tmp files.
# They are used by /usr/local/bin/cio-hadoop-topology which is
# launched from the YARN resourcemanager service, but all environment
# variables are wiped upon service startup (and listing them in /etc/environment
# does not keep them from being wiped).
echo "$WITH_COHO_HADOOP_TOPOLOGY" > /tmp/WITH_COHO_HADOOP_TOPOLOGY

if [ "$WITH_COHO_HADOOP_TOPOLOGY" != "false" ]; then
    echo "Writing container's rack location."
    curl -X PUT -d $CTIME http://127.0.0.1:8500/v1/kv/${LOCATION_IP}
    curl -X PUT -d $CTIME http://127.0.0.1:8500/v1/kv/${LOCATION_NAME}

    echo "$COHO_TENANT" > /tmp/COHO_TENANT
    echo "$COHO_CLUSTER_UUID" > /tmp/COHO_CLUSTER_UUID
    echo "$COHO_NODE_UUID" > /tmp/COHO_NODE_UUID
fi

if [ "$make_local_dir" != "false" ]; then
    local_dir="$local_dir_base/$(cat /proc/sys/kernel/random/uuid)"

    echo "Creating local-dir: $local_dir"
    mkdir -p $local_dir
    # XXX: the nm runs as the hdfs user, which won't have permissions to write
    #      to the localdir unless we bump up the permissions
    chmod 777 $local_dir

    # Update the configuration files
    sed -i "s|LOCAL_DIR_OVERRIDE|${local_dir}|" $local_dir_conf_file
fi

service $service_script start
RESULT=$?

if [ "$service_in_backround" == "false" ]
then
    echo "This service ran in foreground"
else
    (spawn_service_poll &)
    tail -F $service_log_file
    # There is no graceful exit for this service
    RESULT=1
fi

if [ "$make_local_dir" != "false" ]; then
    echo "Removing nm-local-dir: $nm_local_dir"
    rm -rf $nm_local_dir
fi

if [ "$WITH_COHO_HADOOP_TOPOLOGY" != "false" ]; then
    echo "Removing container's rack location."
    curl -X DELETE http://127.0.0.1:8500/v1/kv/${LOCATION_IP}
    curl -X DELETE http://127.0.0.1:8500/v1/kv/${LOCATION_NAME}
fi

exit $RESULT
